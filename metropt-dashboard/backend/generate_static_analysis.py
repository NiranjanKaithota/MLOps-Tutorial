import pandas as pd
import numpy as np
import pickle
import json
import os
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_absolute_error, mean_squared_error

# --- CONFIGURATION ---
BASE_DIR = r'D:\Projects\EL Models\MLOps\MLOps-Tutorial'  # Centralized path
DATA_PATH = os.path.join(BASE_DIR, 'data', 'engineered_data.parquet')

MODELS = {
    'GRU': os.path.join(BASE_DIR, 'gru_model.keras'),
    'LSTM': os.path.join(BASE_DIR, 'lstm_model.keras'),
    'CNN': os.path.join(BASE_DIR, 'cnn_model.keras')
}

# UPDATE: Pointing to the specific X and Y scalers generated by train.py
SCALER_X_PATH = os.path.join(BASE_DIR, 'scaler_X.pkl') 
SCALER_Y_PATH = os.path.join(BASE_DIR, 'scaler_y.pkl') # <--- NEW: Needed to convert predictions back

OUTPUT_FILE = os.path.join(BASE_DIR, 'metropt-dashboard', 'public', 'static_performance.json')

SEQ_LENGTH = 180
STRIDE = 200  # Skip every N steps to keep JSON file size small for web

# 1. LOAD RESOURCES
print("â³ Loading Data & Scalers...")
if not os.path.exists(SCALER_Y_PATH):
    raise FileNotFoundError(f"Could not find {SCALER_Y_PATH}. Make sure you ran the updated train.py!")

df = pd.read_parquet(DATA_PATH)

with open(SCALER_X_PATH, 'rb') as f:
    scaler_x = pickle.load(f)

with open(SCALER_Y_PATH, 'rb') as f:
    scaler_y = pickle.load(f) # <--- Load the target scaler

# 2. SPLIT DATA (Same as training: 75% Train, 25% Test)
split_idx = int(len(df) * 0.75)
train_df = df.iloc[:split_idx].reset_index(drop=True)
test_df = df.iloc[split_idx:].reset_index(drop=True)

print(f"ðŸ“Š Data Split - Train: {len(train_df)}, Test: {len(test_df)}")

# Helper to prepare sequences
def prepare_data(data_df, stride=1):
    feature_cols = [c for c in data_df.columns if c not in ['timestamp', 'failure', 'RUL']]
    
    # Use scaler_x for features
    X_raw = scaler_x.transform(data_df[feature_cols])
    y_raw = data_df['RUL'].values
    timestamps = data_df['timestamp'].values
    
    X, y, ts = [], [], []
    # Create sequences
    for i in range(0, len(X_raw) - SEQ_LENGTH, stride):
        X.append(X_raw[i : i + SEQ_LENGTH])
        y.append(y_raw[i + SEQ_LENGTH])
        ts.append(str(timestamps[i + SEQ_LENGTH]))
        
    return np.array(X), np.array(y), ts

# Prepare Subsampled Data for Visualization
print("ðŸ”„ Creating Sequences (this may take a minute)...")
X_train, y_train, ts_train = prepare_data(train_df, stride=STRIDE)
X_test, y_test, ts_test = prepare_data(test_df, stride=STRIDE)

results = {
    "metrics": {},
    "train_data": [],
    "test_data": []
}

# 3. RUN INFERENCE FOR ALL MODELS
for name, path in MODELS.items():
    if os.path.exists(path):
        print(f"ðŸ§  Running {name} on Train/Test...")
        model = load_model(path)
        
        # Predict (Result is Scaled)
        pred_train_scaled = model.predict(X_train, verbose=0)
        pred_test_scaled = model.predict(X_test, verbose=0)
        
        # INVERSE TRANSFORM (Convert back to Hours)
        # We assume predictions are shape (N, 1), scaler expects (N, 1)
        pred_train = scaler_y.inverse_transform(pred_train_scaled).flatten()
        pred_test = scaler_y.inverse_transform(pred_test_scaled).flatten()
        
        # Calculate Metrics (Now using real hours)
        mae_train = mean_absolute_error(y_train, pred_train)
        mae_test = mean_absolute_error(y_test, pred_test)
        
        results["metrics"][name] = {
            "MAE_Train": round(mae_train, 2),
            "MAE_Test": round(mae_test, 2)
        }
        
        # Store temporary data for JSON building
        if name == 'GRU': results['gru_train'] = pred_train; results['gru_test'] = pred_test
        if name == 'LSTM': results['lstm_train'] = pred_train; results['lstm_test'] = pred_test
        if name == 'CNN': results['cnn_train'] = pred_train; results['cnn_test'] = pred_test
    else:
        print(f"âš ï¸ Model {path} not found. Skipping.")

# 4. STRUCTURE JSON FOR FRONTEND
print("ðŸ’¾ Saving to JSON...")

# Build Train List
for i in range(len(ts_train)):
    entry = {
        "time": ts_train[i],
        "Actual": round(float(y_train[i]), 1)
    }
    # Add model predictions if they exist
    if 'gru_train' in results: entry["GRU"] = round(float(results['gru_train'][i]), 1)
    if 'lstm_train' in results: entry["LSTM"] = round(float(results['lstm_train'][i]), 1)
    if 'cnn_train' in results: entry["CNN"] = round(float(results['cnn_train'][i]), 1)
    
    results["train_data"].append(entry)

# Build Test List
for i in range(len(ts_test)):
    entry = {
        "time": ts_test[i],
        "Actual": round(float(y_test[i]), 1)
    }
    # Add model predictions if they exist
    if 'gru_test' in results: entry["GRU"] = round(float(results['gru_test'][i]), 1)
    if 'lstm_test' in results: entry["LSTM"] = round(float(results['lstm_test'][i]), 1)
    if 'cnn_test' in results: entry["CNN"] = round(float(results['cnn_test'][i]), 1)

    results["test_data"].append(entry)

# Clean up temp arrays
keys_to_remove = ['gru_train', 'gru_test', 'lstm_train', 'lstm_test', 'cnn_train', 'cnn_test']
for k in keys_to_remove:
    results.pop(k, None)

# Ensure directory exists
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

with open(OUTPUT_FILE, 'w') as f:
    json.dump(results, f)

print(f"âœ… Static analysis saved to {OUTPUT_FILE}")